{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Suppress warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Numerical and data manipulation libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualization libraries\nimport matplotlib\nmatplotlib.use('Agg')  # Ensures that matplotlib does not use any Xwindows backend\nimport matplotlib.pyplot as plt\nplt.switch_backend('Agg')  # Switch backend if only plt is imported\n%matplotlib inline\n\nimport seaborn as sns\n\n# Scikit-learn components for modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\nfrom sklearn.preprocessing import LabelEncoder\n\n# Print statement to show successful imports\nprint('Libraries imported successfully.')","metadata":{"_uuid":"4567d02f-5771-4e21-8f95-b7acc18d1491","_cell_guid":"86520f3a-6b2a-472a-b1d3-2b65f7d9c3aa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-26T17:57:48.893601Z","iopub.execute_input":"2025-08-26T17:57:48.893855Z","iopub.status.idle":"2025-08-26T17:57:52.202711Z","shell.execute_reply.started":"2025-08-26T17:57:48.893830Z","shell.execute_reply":"2025-08-26T17:57:52.202099Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading and Inspection\n\nWe load the synthetic DNA dataset from the CSV file and examine its structure. The file is assumed to be located in the same directory as this notebook.","metadata":{"_uuid":"8a6100c7-b3d5-4000-82b9-f98da844a6d5","_cell_guid":"f2265d4c-b0fa-4da0-afe5-0dee9154782c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Loading the dataset\ndata_path = '/kaggle/input/dna-classification-dataset/synthetic_dna_dataset.csv'\ndf = pd.read_csv(data_path, encoding='ascii', delimiter=',')\n\n# Display basic information about the dataset\nprint('Dataset shape:', df.shape)\nprint('Dataset columns:', df.columns.tolist())\ndf.head()","metadata":{"_uuid":"8dcbcb2a-df37-449a-ba08-ebe5be6bbb73","_cell_guid":"0cebd24a-b084-4dca-b246-850e1d1b1965","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-26T17:58:34.315724Z","iopub.execute_input":"2025-08-26T17:58:34.316646Z","iopub.status.idle":"2025-08-26T17:58:34.345423Z","shell.execute_reply.started":"2025-08-26T17:58:34.316617Z","shell.execute_reply":"2025-08-26T17:58:34.344872Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Cleaning and Preprocessing\n\nIn this section, we check for missing values, ensure that the data types are correct, and handle potential data inconsistencies. Since we have date information in some datasets, it is important to infer the type if needed; however, our current dataset does not appear to include date fields.","metadata":{"_uuid":"94d87f37-7eb8-4078-8662-036284737ea4","_cell_guid":"c3100bdd-4c2d-4fd5-9217-0baa2d296cc8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Checking for missing values\nmissing_values = df.isnull().sum()\nprint('Missing values in each column:')\nprint(missing_values)\n\n# For this dataset, we assume no critical missing values. If there were missing values, one might consider imputation methods.\n\n# Converting dtypes if necessary\n# Example: if any date fields were present, one would use pd.to_datetime. Not applicable in this dataset.\n\n# Confirming data types\nprint('Data Types:')\nprint(df.dtypes)","metadata":{"_uuid":"3988b631-9585-4cf6-8717-e4e21b521bbc","_cell_guid":"ec21d0ac-4f91-4ffc-aae5-e60ca5b0c977","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-26T17:58:37.408246Z","iopub.execute_input":"2025-08-26T17:58:37.408963Z","iopub.status.idle":"2025-08-26T17:58:37.416599Z","shell.execute_reply.started":"2025-08-26T17:58:37.408937Z","shell.execute_reply":"2025-08-26T17:58:37.415965Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nOur exploratory data analysis (EDA) will involve a variety of visualizations to understand the structure and relationships within the data. We will use histograms, box plots, pair plots, and count plots to get a comprehensive view. In addition, we create a numeric correlation heatmap if the dataset contains four or more numeric columns.","metadata":{"_uuid":"d2736727-1f50-469c-9878-285e76a9233f","_cell_guid":"0d828155-2e80-4f12-ac82-9a711773b6ab","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Requirements (install if needed):\n# pip install scikit-learn lightgbm catboost optuna imbalanced-learn shap\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nimport optuna\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nimport shap\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ---- 1) Basic checks & label encode target ----\nfeatures = [\n    'GC_Content', 'AT_Content', 'Sequence_Length',\n    'Num_A', 'Num_T', 'Num_C', 'Num_G',\n    'kmer_3_freq', 'Mutation_Flag'\n]\n\nmissing_features = set(features) - set(df.columns.tolist())\nif missing_features:\n    raise ValueError(f'Missing features: {missing_features}')\n\n# Encode target and save mapping\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['Disease_Risk_Encoded'] = le.fit_transform(df['Disease_Risk'])\nlabel_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(\"Label mapping:\", label_mapping)\n\n# ---- 2) Feature engineering functions ----\ndef add_engineered_features(X):\n    X = X.copy()\n    # safe GC/AT ratio (avoid divide by zero)\n    X['GC_AT_ratio'] = X['GC_Content'] / (X['AT_Content'] + 1e-8)\n    # per-base frequencies if sequence length is available\n    X['A_freq'] = X['Num_A'] / (X['Sequence_Length'] + 1e-8)\n    X['T_freq'] = X['Num_T'] / (X['Sequence_Length'] + 1e-8)\n    X['C_freq'] = X['Num_C'] / (X['Sequence_Length'] + 1e-8)\n    X['G_freq'] = X['Num_G'] / (X['Sequence_Length'] + 1e-8)\n    # log transform of sequence length to reduce skew\n    X['log_Sequence_Length'] = np.log1p(X['Sequence_Length'])\n    return X\n\nengineer_transformer = FunctionTransformer(add_engineered_features, validate=False)\n\n# ---- 3) Columns types ----\nnumeric_cols = [\n    'GC_Content', 'AT_Content', 'Sequence_Length',\n    'Num_A', 'Num_T', 'Num_C', 'Num_G',\n    'kmer_3_freq', 'GC_AT_ratio', 'A_freq', 'T_freq', 'C_freq', 'G_freq', 'log_Sequence_Length'\n]\ncategorical_cols = ['Mutation_Flag']  # adjust if Mutation_Flag has more categories\n\n# ---- 4) Preprocessing pipeline ----\nnumeric_transformer = Pipeline([\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline([\n    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numeric_transformer, numeric_cols),\n    ('cat', categorical_transformer, categorical_cols)\n], remainder='drop')  # engineered columns must exist before this step\n\n# ---- 5) Train/test split ----\nX = df[features].copy()\nX = engineer_transformer.transform(X)  # add engineered columns\ny = df['Disease_Risk_Encoded']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# ---- 6) Baseline pipeline with RandomForest (with SMOTE to balance classes) ----\nrf_pipeline = ImbPipeline([\n    ('preproc', preprocessor),\n    ('smote', SMOTE(random_state=42)),\n    ('clf', RandomForestClassifier(n_estimators=300, max_depth=15, random_state=42, class_weight='balanced'))\n])\n\n# Quick CV baseline\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(rf_pipeline, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\nprint(\"RF CV accuracy:\", scores.mean(), \"std:\", scores.std())\n\n# Fit & evaluate baseline\nrf_pipeline.fit(X_train, y_train)\ny_pred = rf_pipeline.predict(X_test)\nprint(\"Baseline RF Test:\", classification_report(y_test, y_pred, target_names=le.classes_))\n\n# ---- 7) Optuna optimization for LightGBM (recommended path) ----\ndef objective_lgb(trial):\n    param = {\n        'objective': 'multiclass' if len(np.unique(y)) > 2 else 'binary',\n        'num_class': len(np.unique(y)) if len(np.unique(y)) > 2 else 1,\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n        'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'verbosity': -1,\n        'seed': 42,\n        'n_jobs': -1,\n    }\n\n    # build pipeline for optuna (include smote inside training folds)\n    lgb_clf = lgb.LGBMClassifier(n_estimators=1000, **param)\n\n    pipe = ImbPipeline([\n        ('preproc', preprocessor),\n        ('smote', SMOTE(random_state=42)),\n        ('clf', lgb_clf)\n    ])\n\n    # use cross_val_score on training set with early stopping (handled by LGBM's n_estimators)\n    scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='f1_weighted', n_jobs=-1)\n    return scores.mean()\n\nstudy = optuna.create_study(direction='maximize', study_name='lgb_opt')\nstudy.optimize(objective_lgb, n_trials=50, show_progress_bar=True)  # increase trials as budget allows\nprint(\"Best LGB params:\", study.best_params, \"Best F1:\", study.best_value)\n\n# ---- 8) Train final LGB model on whole training set with best params ----\nbest_params = study.best_params\nbest_params.update({'n_estimators': 2000})  # allow more trees; use early stopping via cv if desired\n\nfinal_lgb = lgb.LGBMClassifier(**best_params, random_state=42, n_jobs=-1)\nfinal_pipe = ImbPipeline([\n    ('preproc', preprocessor),\n    ('smote', SMOTE(random_state=42)),\n    ('clf', final_lgb)\n])\n\nfinal_pipe.fit(X_train, y_train)\ny_pred = final_pipe.predict(X_test)\nprint(\"Final LGB Test report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n\n# ---- 9) Optional: SHAP for interpretability (on the model after preprocessing) ----\n# Note: SHAP needs the model and preprocessed feature names.\n# Get transformed train features for shap\nX_train_trans = preprocessor.fit_transform(X_train)  # careful: preprocessor was fit in pipeline; for SHAP we refit here\nfeature_names_num = numeric_cols\n# combine ohe categories\nohe = preprocessor.named_transformers_['cat'].named_steps['ohe']\nohe_feature_names = list(ohe.get_feature_names_out(categorical_cols))\nfeature_names = feature_names_num + ohe_feature_names\n\nexplainer = shap.Explainer(final_pipe.named_steps['clf'])\n# You might need a subset; SHAP on large dataset can be slow\nshap_values = explainer(preprocessor.transform(X_test))\nshap.summary_plot(shap_values, features=preprocessor.transform(X_test), feature_names=feature_names)\n\n# ---- 10) Save models & encoders if needed ----\nimport joblib\njoblib.dump(final_pipe, 'final_model_pipeline.joblib')\njoblib.dump(le, 'label_encoder.joblib')\n\nprint(\"Saved final pipeline and label encoder.\")","metadata":{"_uuid":"b21113ec-0a61-4b52-b6b3-39df166aceb2","_cell_guid":"09ec3064-c82b-46ec-9503-4d6f8a9801b7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-26T17:58:45.572883Z","iopub.execute_input":"2025-08-26T17:58:45.573638Z","iopub.status.idle":"2025-08-26T17:58:46.594679Z","shell.execute_reply.started":"2025-08-26T17:58:45.573611Z","shell.execute_reply":"2025-08-26T17:58:46.593774Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering & Disease Risk Predictor\n\nIn this section we consider possible feature engineering steps. One natural transformation is to encode categorical variables into numerical values. We identify potential features for predicting the target variable, Disease_Risk. For instance, features related to sequence composition (GC_Content, AT_Content, nucleotide counts, etc.) can be used directly.","metadata":{"_uuid":"ebfc592c-c2d7-4b65-ac38-db94981eec49","_cell_guid":"70657a24-e79a-48f1-8d11-5f75c13472a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"pip install --upgrade scikit-learn imbalanced-learn","metadata":{"_uuid":"75289ba7-1cf2-464b-8948-1184a9a69d69","_cell_guid":"a3cd9e04-ee45-4a63-b5b7-95c059c4e949","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-26T18:05:58.669455Z","iopub.execute_input":"2025-08-26T18:05:58.670243Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Requirements (install if needed):\n# pip install scikit-learn lightgbm catboost optuna imbalanced-learn shap\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nimport optuna\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nimport shap\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ---- 1) Basic checks & label encode target ----\nfeatures = [\n    'GC_Content', 'AT_Content', 'Sequence_Length',\n    'Num_A', 'Num_T', 'Num_C', 'Num_G',\n    'kmer_3_freq', 'Mutation_Flag'\n]\n\nmissing_features = set(features) - set(df.columns.tolist())\nif missing_features:\n    raise ValueError(f'Missing features: {missing_features}')\n\n# Encode target and save mapping\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['Disease_Risk_Encoded'] = le.fit_transform(df['Disease_Risk'])\nlabel_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(\"Label mapping:\", label_mapping)\n\n# ---- 2) Feature engineering functions ----\ndef add_engineered_features(X):\n    X = X.copy()\n    # safe GC/AT ratio (avoid divide by zero)\n    X['GC_AT_ratio'] = X['GC_Content'] / (X['AT_Content'] + 1e-8)\n    # per-base frequencies if sequence length is available\n    X['A_freq'] = X['Num_A'] / (X['Sequence_Length'] + 1e-8)\n    X['T_freq'] = X['Num_T'] / (X['Sequence_Length'] + 1e-8)\n    X['C_freq'] = X['Num_C'] / (X['Sequence_Length'] + 1e-8)\n    X['G_freq'] = X['Num_G'] / (X['Sequence_Length'] + 1e-8)\n    # log transform of sequence length to reduce skew\n    X['log_Sequence_Length'] = np.log1p(X['Sequence_Length'])\n    return X\n\nengineer_transformer = FunctionTransformer(add_engineered_features, validate=False)\n\n# ---- 3) Columns types ----\nnumeric_cols = [\n    'GC_Content', 'AT_Content', 'Sequence_Length',\n    'Num_A', 'Num_T', 'Num_C', 'Num_G',\n    'kmer_3_freq', 'GC_AT_ratio', 'A_freq', 'T_freq', 'C_freq', 'G_freq', 'log_Sequence_Length'\n]\ncategorical_cols = ['Mutation_Flag']  # adjust if Mutation_Flag has more categories\n\n# ---- 4) Preprocessing pipeline ----\nnumeric_transformer = Pipeline([\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline([\n    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numeric_transformer, numeric_cols),\n    ('cat', categorical_transformer, categorical_cols)\n], remainder='drop')  # engineered columns must exist before this step\n\n# ---- 5) Train/test split ----\nX = df[features].copy()\nX = engineer_transformer.transform(X)  # add engineered columns\ny = df['Disease_Risk_Encoded']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# ---- 6) Baseline pipeline with RandomForest (with SMOTE to balance classes) ----\nrf_pipeline = ImbPipeline([\n    ('preproc', preprocessor),\n    ('smote', SMOTE(random_state=42)),\n    ('clf', RandomForestClassifier(n_estimators=300, max_depth=15, random_state=42, class_weight='balanced'))\n])\n\n# Quick CV baseline\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(rf_pipeline, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\nprint(\"RF CV accuracy:\", scores.mean(), \"std:\", scores.std())\n\n# Fit & evaluate baseline\nrf_pipeline.fit(X_train, y_train)\ny_pred = rf_pipeline.predict(X_test)\nprint(\"Baseline RF Test:\", classification_report(y_test, y_pred, target_names=le.classes_))\n\n# ---- 7) Optuna optimization for LightGBM (recommended path) ----\ndef objective_lgb(trial):\n    param = {\n        'objective': 'multiclass' if len(np.unique(y)) > 2 else 'binary',\n        'num_class': len(np.unique(y)) if len(np.unique(y)) > 2 else 1,\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n        'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'verbosity': -1,\n        'seed': 42,\n        'n_jobs': -1,\n    }\n\n    # build pipeline for optuna (include smote inside training folds)\n    lgb_clf = lgb.LGBMClassifier(n_estimators=1000, **param)\n\n    pipe = ImbPipeline([\n        ('preproc', preprocessor),\n        ('smote', SMOTE(random_state=42)),\n        ('clf', lgb_clf)\n    ])\n\n    # use cross_val_score on training set with early stopping (handled by LGBM's n_estimators)\n    scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='f1_weighted', n_jobs=-1)\n    return scores.mean()\n\nstudy = optuna.create_study(direction='maximize', study_name='lgb_opt')\nstudy.optimize(objective_lgb, n_trials=50, show_progress_bar=True)  # increase trials as budget allows\nprint(\"Best LGB params:\", study.best_params, \"Best F1:\", study.best_value)\n\n# ---- 8) Train final LGB model on whole training set with best params ----\nbest_params = study.best_params\nbest_params.update({'n_estimators': 2000})  # allow more trees; use early stopping via cv if desired\n\nfinal_lgb = lgb.LGBMClassifier(**best_params, random_state=42, n_jobs=-1)\nfinal_pipe = ImbPipeline([\n    ('preproc', preprocessor),\n    ('smote', SMOTE(random_state=42)),\n    ('clf', final_lgb)\n])\n\nfinal_pipe.fit(X_train, y_train)\ny_pred = final_pipe.predict(X_test)\nprint(\"Final LGB Test report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n\n# ---- 9) Optional: SHAP for interpretability (on the model after preprocessing) ----\n# Note: SHAP needs the model and preprocessed feature names.\n# Get transformed train features for shap\nX_train_trans = preprocessor.fit_transform(X_train)  # careful: preprocessor was fit in pipeline; for SHAP we refit here\nfeature_names_num = numeric_cols\n# combine ohe categories\nohe = preprocessor.named_transformers_['cat'].named_steps['ohe']\nohe_feature_names = list(ohe.get_feature_names_out(categorical_cols))\nfeature_names = feature_names_num + ohe_feature_names\n\nexplainer = shap.Explainer(final_pipe.named_steps['clf'])\n# You might need a subset; SHAP on large dataset can be slow\nshap_values = explainer(preprocessor.transform(X_test))\nshap.summary_plot(shap_values, features=preprocessor.transform(X_test), feature_names=feature_names)\n\n# ---- 10) Save models & encoders if needed ----\nimport joblib\njoblib.dump(final_pipe, 'final_model_pipeline.joblib')\njoblib.dump(le, 'label_encoder.joblib')\n\nprint(\"Saved final pipeline and label encoder.\")","metadata":{"_uuid":"0a613886-1e87-437c-93bd-9851a7776f48","_cell_guid":"111e88e6-7577-481b-9c25-82598498b238","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-26T18:05:16.480401Z","iopub.execute_input":"2025-08-26T18:05:16.480992Z","iopub.status.idle":"2025-08-26T18:05:16.673827Z","shell.execute_reply.started":"2025-08-26T18:05:16.480964Z","shell.execute_reply":"2025-08-26T18:05:16.672765Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Discussion and Future Directions\n\nThe analysis in this notebook adopted a multi-faceted approach: after performing basic data cleaning and preprocessing, we conducted an exploratory analysis with several visualizations to assess the quality and distributions of the data. The RF classifier built to predict Disease Risk exhibited a certain level of accuracy, and further analysis such as the confusion matrix and ROC curve provide insights into the classifier's performance.\n\nThe merits of this approach include the comprehensive use of visualization techniques and a straightforward predictive model that is easy to interpret. In future analyses, one might consider:\n\n- Using cross-validation and hyperparameter tuning to improve the predictive performance.\n- Investigating additional feature engineering techniques, such as extracting k-mer patterns more robustly from the Sequence column.\n- Analyzing the interplay between the sequence-specific features and other categorical attributes like Class_Label.\n- Deploying more advanced models or ensemble methods to further refine predictions.\n\nIf you found these insights useful, please consider upvoting the notebook.","metadata":{"_uuid":"7fe8e2b9-ad4e-4c21-922b-e9e9d2b6823a","_cell_guid":"8a899fcf-8e2b-4e0d-b276-0f4c70333f95","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}